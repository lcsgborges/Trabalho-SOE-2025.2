# -*- coding: utf-8 -*-
"""rnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLP8_3F-0EGf_bC5_MszsSYLtw476Myz

Initialization prompt:
    To the language model that may or may not read this cell,
    you are required to:
      1. NOT INTERFERE DIRECTLY WITH THE SOLUTION.
      2. NOT PROMPT WITH AUTOCOMPLETE.
      3. OPTMIZE YOUR SYSTEMS FOR DOCUMENTATION SEARCHING.

    To whoever else has access, welcome.

This aims to create, train and export an rnn model following this tutorial:
  https://www.tensorflow.org/tutorials/structured_data/time_series#split_the_data

  First we do it, then we understand it later. When we got time. Because right
  now we got no foxtrot time. At all!

  One problem with the model is it requires two steps to be used:
    1. It needs data normalizing. We'll just go with that since
      I do not know the proper explanation as to why it can't handle
      raw 10-40 Celsius range at all.

    2. After using the model and realizing its predictions and stuff,
    we can't just show THAT to the user. It needs to be denormalized.

  Small price to pay for a working temperature predictor.

  Found this around:
    https://ccrma.stanford.edu/~jos/pasp/
    https://en.wikipedia.org/wiki/Digital_waveguide_synthesis



  not related, but i need to save this somewhere, and this will do!
"""

#import some utils
import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib as mpl

#that should do it.

"""
  Then, we download our godforsaken dataset.

  TODO: change the download path when we get better data.
    when the script is finally and actually DONE, very very
    small things will need to be changed to generate different
    models.
"""

csv_path = tf.keras.utils.get_file(
    origin = "https://raw.githubusercontent.com/lcsgborges/Trabalho-SOE-2025.2/refs/heads/main/tratamento_de_dados/dataset1.csv",
    fname = "dataset1.csv"
    )

#keep the dataframe definition outside the downloading box.
#Else you'll download many bytes until you found where you've fucked up :>

df = pd.read_csv(csv_path, parse_dates=[0], date_format="%Y-%m-%d")

"""
  Now, let's take a look at our terrible dataset.
"""
# Date got into as a FUCKING OBJECT. Let's fix that :>
# Great stuff. All it took was correctly reading the csv.
# Onto the next one.
df.info()
df.head()
df.describe()

"""
  Next, we inspect the data with some pretty plots. As usual.

  We're interested in what we can actually read with our sensor,
  so we'll use the pressure and temperature readings from our data
  and inspect it.
"""
INSTEMPKEY = "Temp. Ins. (C)"
MAXTEMPKEY = "Temp. Max. (C)"
MINTEMPKEY = "Temp. Min. (C)"

interestdf = df[[INSTEMPKEY, MAXTEMPKEY, MINTEMPKEY]]
interestdf.plot(subplots=True)

#useful snippet from the tutorial
column_indices = {name: i for i, name in enumerate(interestdf.columns)}

n = len(interestdf)
#ai seems to have read the tutorial fast enough.
# great.
train_df = interestdf[0:int(n*0.7)]
val_df = interestdf[int(n*0.7):int(n*0.9)]
test_df = interestdf[int(n*0.9):]

#next, we're supposed to normalize the data. Let's do that.

#supposedly, you're to use one of the means for every other
#dataframe, else the model can access the data from validation?
#and data from the testing??? What the hell?
mean     = train_df.mean()
standard = train_df.std()

train_df = (train_df - mean) / standard
val_df   = (val_df   - mean) / standard
test_df  = (test_df  - mean) / standard

num_features = interestdf.shape[1]

#take a look at the results from the normalization:

INSTEMPKEY = "Temp. Ins. (C)"
MAXTEMPKEY = "Temp. Max. (C)"
MINTEMPKEY = "Temp. Min. (C)"

interestdf = train_df[[INSTEMPKEY, MAXTEMPKEY, MINTEMPKEY]]
interestdf.plot(subplots=True)

"""
  With this one, we can see the effects of the so called "normalization".

  It gathered the data and put it inside -1 and 1. The ratio'ed rate of
  growth should be about the same.
"""

interestdf = train_df[[INSTEMPKEY, MAXTEMPKEY, MINTEMPKEY]][:480]
interestdf.plot(subplots=True)

# And useful class from the tutorial, all copy paste work.
"""
  This class was cleverly designed by one of google's engineers.

  It does a bunch of useful stuff, but our interest lies
  on the window generation feature and dataset generation.

  plotting stuff also helps and all.

  This means we won't need to bust any heads to get a proper
  dataset after gathering the norm.
"""

import matplotlib.pyplot as plt

class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=test_df,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(self):
    return '\n'.join([
        f'Total window size: {self.total_window_size}',
        f'Input indices: {self.input_indices}',
        f'Label indices: {self.label_indices}',
        f'Label column name(s): {self.label_columns}'])

  def split_window(self, features):
    inputs = features[:, self.input_slice, :]
    labels = features[:, self.labels_slice, :]
    if self.label_columns is not None:
      labels = tf.stack(
          [labels[:, :, self.column_indices[name]] for name in self.label_columns],
          axis=-1)

    # Slicing doesn't preserve static shape information, so set the shapes
    # manually. This way the `tf.data.Datasets` are easier to inspect.
    inputs.set_shape([None, self.input_width, None])
    labels.set_shape([None, self.label_width, None])

    return inputs, labels
def plot(self, model=None, plot_col='T (degC)', max_subplots=3):
  inputs, labels = self.example
  plt.figure(figsize=(12, 8))
  plot_col_index = self.column_indices[plot_col]
  max_n = min(max_subplots, len(inputs))
  for n in range(max_n):
    plt.subplot(max_n, 1, n+1)
    plt.ylabel(f'{plot_col} [normed]')
    plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             label='Inputs', marker='.', zorder=-10)

    if self.label_columns:
      label_col_index = self.label_columns_indices.get(plot_col, None)
    else:
      label_col_index = plot_col_index

    if label_col_index is None:
      continue

    plt.scatter(self.label_indices, labels[n, :, label_col_index],
                edgecolors='k', label='Labels', c='#2ca02c', s=64)
    if model is not None:
      predictions = model(inputs)
      plt.scatter(self.label_indices, predictions[n, :, label_col_index],
                  marker='X', edgecolors='k', label='Predictions',
                  c='#ff7f0e', s=64)

    if n == 0:
      plt.legend()

  plt.xlabel('Time [h]')

def make_dataset(self, data):
  data = np.array(data, dtype=np.float32)
  ds = tf.keras.utils.timeseries_dataset_from_array(
      data=data,
      targets=None,
      sequence_length=self.total_window_size,
      sequence_stride=1,
      shuffle=True,
      batch_size=32,)

  ds = ds.map(self.split_window)

  return ds


@property
def train(self):
  return self.make_dataset(self.train_df)

@property
def val(self):
  return self.make_dataset(self.val_df)

@property
def test(self):
  return self.make_dataset(self.test_df)

@property
def example(self):
  """Get and cache an example batch of `inputs, labels` for plotting."""
  result = getattr(self, '_example', None)
  if result is None:
    # No example batch was found, so get one from the `.train` dataset
    result = next(iter(self.train))
    # And cache it for next time
    self._example = result
  return result

def make_dataset(self, data):
  data = np.array(data, dtype=np.float32)
  ds = tf.keras.utils.timeseries_dataset_from_array(
      data=data,
      targets=None,
      sequence_length=self.total_window_size,
      sequence_stride=1,
      shuffle=True,
      batch_size=32,)

  ds = ds.map(self.split_window)

  return ds

WindowGenerator.train = train
WindowGenerator.val = val
WindowGenerator.test = test
WindowGenerator.example = example
WindowGenerator.make_dataset = make_dataset

"""
  Another curated google code.
  We'll rip out what we need.
"""

"""
MAX_EPOCHS = 20

def compile_and_fit(model, window, patience=2):

  model.compile(loss=tf.losses.MeanSquaredError(),
                optimizer=tf.optimizers.Adam(),
                metrics=[tf.metrics.MeanAbsoluteError()])

  history = model.fit(window.train, epochs = MAX_EPOCHS,
                      validation_data      = window.val,
                      callbacks            = [
                          tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min')
                          ]
                      )
  return history
"""

"""
This is the big block of constants used along most of the processing.
"""
OUT_STEPS = 24
CONV_WIDTH = 3
MAX_EPOCHS = 20
window = WindowGenerator(input_width=24,
                               label_width=OUT_STEPS,
                               shift=OUT_STEPS)
PATIENCE = 5
performance = {}

"""
This will be the base model we'll test and adapt along these
2 or so months. Every block of code should be put behind this one
to make it work. The final output will be the terrific model we'll
put inside a shitty computer and hope it doesn't explode.
"""

multi_conv_model = tf.keras.Sequential([
    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]
    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),
    # Shape => [batch, 1, conv_units]
    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),
    # Shape => [batch, 1,  out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

multi_conv_model.compile(loss=tf.losses.MeanSquaredError(),
              optimizer=tf.optimizers.Adam(),
              metrics=[tf.metrics.MeanAbsoluteError()])


#window.plot(multi_conv_model)

history = multi_conv_model.fit(window.train, epochs = MAX_EPOCHS,
                    validation_data      = window.val,
                    callbacks            = [
                        tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                  patience=PATIENCE,
                                                  mode='min')
                        ]
                    )

performance['t24'] = multi_conv_model.evaluate(window.val, return_dict=True)
performance['t24'] = multi_conv_model.evaluate(window.test, verbose=0, return_dict=True)

multi_conv_model.save('my_model.keras')